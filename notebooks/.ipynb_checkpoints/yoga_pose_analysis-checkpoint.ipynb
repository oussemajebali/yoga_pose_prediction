{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "The goal of this project is to classify yoga poses into categories using deep learning. \n",
    "Specifically, we want to analyze images of people performing yoga poses (e.g., downdog, goddess pose) \n",
    "and predict the correct pose category using three deep learning architectures:\n",
    "- Two architectures built from scratch\n",
    "- One pre-trained architecture (e.g., MobileNetV2)\n",
    "\n",
    "We will evaluate the performance of these models to identify the best approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Requirements & Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train folders: ['downdog', 'goddess']\n",
      "Test folders: ['downdog', 'goddess']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Dataset paths\n",
    "train_dir = '../DATASET/TRAIN/'\n",
    "test_dir = '../DATASET/TEST/'\n",
    "\n",
    "# Check train subfolders\n",
    "print(\"Train folders:\", os.listdir(train_dir))\n",
    "print(\"Test folders:\", os.listdir(test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Image dimensions\u001b[39;00m\n\u001b[0;32m      4\u001b[0m img_height, img_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m150\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image dimensions\n",
    "img_height, img_width = 150, 150\n",
    "batch_size = 32\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train and validation data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical', subset='training')\n",
    "\n",
    "val_data = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical', subset='validation')\n",
    "\n",
    "# Test data\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# First architecture (Simple CNN)\n",
    "model1 = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_data.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history1 = model1.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Conv2D(64, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_data.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history2 = model2.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(train_data.num_classes, activation='softmax')(x)\n",
    "\n",
    "model3 = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history3 = model3.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on the test data\n",
    "print(\"Model 1 Accuracy:\", model1.evaluate(test_data)[1])\n",
    "print(\"Model 2 Accuracy:\", model2.evaluate(test_data)[1])\n",
    "print(\"Model 3 Accuracy:\", model3.evaluate(test_data)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
